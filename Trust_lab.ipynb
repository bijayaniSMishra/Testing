{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trust lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z7vvPXJZfw-"
      },
      "source": [
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import re\n",
        "import requests\n",
        "import sys\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImVycVHFZz3_",
        "outputId": "1a304a25-383c-4056-b88c-ef1b8c13b638"
      },
      "source": [
        "# !pip install warcio\n",
        "!pip install beautifulsoup4\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4) (2.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTcPYpBlZmtY"
      },
      "source": [
        "# regex = re.compile(\n",
        "#     \"(covid\\.be/|corona\\.com/(Virus\\?(.*\\&)?v=|(economy|v)/))([^?&\\\"'>]+)\"\n",
        "# )\n",
        "\n",
        "#https://arxiv.org/pdf/2102.09507.pdf\n",
        "#https://towardsdatascience.com/web-scraping-news-articles-in-python-9dd605799558 \n",
        "all_url = set()\n",
        "def get_url(seg_url):\n",
        "    regex = re.compile(\n",
        "        \"((?=.*pandemic*)|(?=.*covid*)|(?=.*lockdown*)) \"\n",
        "    )\n",
        "\n",
        "    entries = 0\n",
        "    matching_entries = 0\n",
        "    hits = 0\n",
        "\n",
        "    #file_name = \"http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-30/segments/1563195523840.34/warc/CC-MAIN-20190715175205-20190715200159-00000.warc.gz\"\n",
        "\n",
        "    file_name = \"https://commoncrawl.s3.amazonaws.com/\" + seg_url\n",
        "\n",
        "    # if len(sys.argv) > 1:\n",
        "    #     file_name = sys.argv[1]\n",
        "\n",
        "    stream = None\n",
        "\n",
        "    if file_name.startswith(\"http://\") or file_name.startswith(\n",
        "        \"https://\"\n",
        "    ):\n",
        "        stream = requests.get(file_name, stream=True).raw\n",
        "    else:\n",
        "        stream = open(file_name, 'rb')\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for record in ArchiveIterator(stream):\n",
        "        content_str = \"\"\n",
        "        if record.rec_type == \"warcinfo\":\n",
        "            continue\n",
        "        #print(record.content_stream())\n",
        "        #break\n",
        "        if not \".com/\" in record.rec_headers.get_header(\n",
        "            \"WARC-Target-URI\"\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        entries = entries + 1\n",
        "        contents = (\n",
        "            record.content_stream()\n",
        "            .read()\n",
        "            .decode(\"utf-8\", \"replace\")\n",
        "        )\n",
        "        url  = record.rec_headers.get_header('WARC-Target-URI')\n",
        "\n",
        "        # soup = BeautifulSoup(contents, 'html5lib')\n",
        "        # htmlcontent = soup.find_all()\n",
        "        # for elem in htmlcontent:\n",
        "        #   content_str += elem.get_text()\n",
        "        #   count +=1\n",
        "    \n",
        "        m = regex.search(contents,  re.IGNORECASE)\n",
        "        if m:\n",
        "            matching_entries = matching_entries + 1\n",
        "            hits = hits + 1\n",
        "            # m = regex.search(content_str, m.end())\n",
        "\n",
        "\n",
        "        while m:\n",
        "          # m = regex.search(content_str, m.end())\n",
        "            hits = hits + 1\n",
        "            all_url.append(url)\n",
        "            print(url)\n",
        "      \n",
        "        if len(all_url) == 1000:\n",
        "          break\n",
        "\n",
        "    print(\n",
        "        \"Python: \"\n",
        "        + str(hits)\n",
        "        + \" matches in \"\n",
        "        + str(matching_entries)\n",
        "        + \"/\"\n",
        "        + str(entries)\n",
        "    )\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dcEhfIvde8q"
      },
      "source": [
        "\n",
        "Year_2020_url = ['crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00177.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00178.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00179.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00180.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00181.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00182.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00183.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00184.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00185.warc.gz',\n",
        "'crawl-data/CC-MAIN-2020-45/segments/1603107866404.1/warc/CC-MAIN-20201019203523-20201019233523-00186.warc.gz']\n",
        "\n",
        "for url in Year_2020_url:\n",
        "  get_url(url)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFKdL-S0h_Sz"
      },
      "source": [
        "print(len(all_url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odtv72eWZ4uA"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "from scipy import spatial\n",
        "\n",
        "# method for searching keyword from the text\n",
        "def search_for_keyword(keyword, doc_obj, nlp):\n",
        "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
        "    phrase_list = [nlp(keyword)]\n",
        "    phrase_matcher.add(\"Text Extractor\", None, *phrase_list)\n",
        "\n",
        "    matched_items = phrase_matcher(doc_obj)\n",
        "\n",
        "    matched_text = []\n",
        "    for match_id, start, end in matched_items:\n",
        "        text = nlp.vocab.strings[match_id]\n",
        "        span = doc_obj[start: end]\n",
        "        matched_text.append(span.sent.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOrqjEunx2kk",
        "outputId": "d7530c8a-3b7f-4a78-8a96-64e1b596bd4f"
      },
      "source": [
        "!git clone https://github.com/EdwardJRoss/job-advert-analysis.git "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'job-advert-analysis'...\n",
            "remote: Enumerating objects: 729, done.\u001b[K\n",
            "remote: Counting objects: 100% (729/729), done.\u001b[K\n",
            "remote: Compressing objects: 100% (367/367), done.\u001b[K\n",
            "remote: Total 729 (delta 484), reused 593 (delta 349), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (729/729), 977.47 KiB | 14.37 MiB/s, done.\n",
            "Resolving deltas: 100% (484/484), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HSKxo4wz0DY",
        "outputId": "2c1e507d-7e85-4ba0-ce23-ea5818e0b8bb"
      },
      "source": [
        "pip install mypy-extensions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mypy-extensions\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: mypy-extensions\n",
            "Successfully installed mypy-extensions-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_6p39giyC5N"
      },
      "source": [
        "class CommonCrawlDatasource():\n",
        "\n",
        "    query: str\n",
        "    query_filters: List[str] = []\n",
        "    # Number of threads for parallel\n",
        "   #nthread: int = _DEFAULT_NTHREAD\n",
        "    disable_progress: bool = False\n",
        "\n",
        "    raw_extension = \".warc.gz\"\n",
        "\n",
        "    sources = {\n",
        "        \"CC-MAIN-2020-50\": \"https://index.commoncrawl.org/CC-MAIN-2020-50-index\",\n",
        "        \"CC-MAIN-2020-45\": \"https://index.commoncrawl.org/CC-MAIN-2020-45-index\",\n",
        "        \"CC-MAIN-2020-40\": \"https://index.commoncrawl.org/CC-MAIN-2020-40-index\",\n",
        "        \"CC-MAIN-2020-34\": \"https://index.commoncrawl.org/CC-MAIN-2020-34-index\",\n",
        "        \"CC-MAIN-2020-29\": \"https://index.commoncrawl.org/CC-MAIN-2020-29-index\",\n",
        "        \"CC-MAIN-2020-24\": \"https://index.commoncrawl.org/CC-MAIN-2020-24-index\",\n",
        "        \"CC-MAIN-2020-16\": \"https://index.commoncrawl.org/CC-MAIN-2020-16-index\",\n",
        "        \"CC-MAIN-2020-10\": \"https://index.commoncrawl.org/CC-MAIN-2020-10-index\",\n",
        "        \"CC-MAIN-2020-05\": \"https://index.commoncrawl.org/CC-MAIN-2020-05-index\",\n",
        "    }\n",
        "\n",
        "    def fetch_source_rows(self, source: str) -> List[CrawlResultDict]:\n",
        "        return fetch_source_rows(source, self.query, self.query_filters)\n",
        "\n",
        "    def download_one(self, path: Path, source: str) -> None:\n",
        "        logging.info(f\"Fetching {source} for {self.name}\")\n",
        "        source_rows = self.fetch_source_rows(source)\n",
        "        with AtomicFileWriter(path) as output:\n",
        "            writer = WARCWriter(output, gzip=True)\n",
        "            logging.info(f\"Downloading {source}\")\n",
        "            for warc in fetch_all_cc(source_rows, self.nthread, self.disable_progress):\n",
        "                writer.write_record(warc)\n",
        "\n",
        "    def extract(self, html: bytes, uri: str, view_date: str) -> List[Dict[Any, Any]]:\n",
        "        pass\n",
        "\n",
        "    def extract_one(self, path: Path) -> Generator[Dict[Any, Any], None, None]:\n",
        "        for warc in read_warc_responses(path):\n",
        "            html = warc.content_stream().read()\n",
        "            uri = warc.rec_headers[\"WARC-Target-URI\"]\n",
        "            view_date = warc.rec_headers[\"WARC-Date\"]\n",
        "\n",
        "            assert uri is not None\n",
        "            assert view_date is not None\n",
        "\n",
        "            for result in self.extract(html, uri, view_date):\n",
        "                yield result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXd06cON0frQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}